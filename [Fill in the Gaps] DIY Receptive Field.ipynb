{"cells":[{"cell_type":"markdown","id":"bc8e7afd","metadata":{"id":"bc8e7afd"},"source":["# To be able to edit and use this Notebook:\n","\n","0. If you have zero python experience, go through a [basic python tutorial](https://colab.research.google.com/github/jckantor/CBE30338/blob/master/docs/01.02-Python-Basics.ipynb)\n","0. Learn about how to use google colaboratory [video](https://www.youtube.com/watch?v=inN8seMm7UI)\n","1. in the file menu (top left), click ```open in playground```\n","3. still in the file menu, click ```save copy in drive```, to make your own personalized and editable copy of this file.\n","4. edit as you like. If something breaks irreparably, either:\n","  1. restart the ```Runtime```\n","  2. or go back to step 1.\n"]},{"cell_type":"markdown","id":"4872a426","metadata":{"id":"4872a426"},"source":["# Project: DIY Receptive Field (FFNN)"]},{"cell_type":"markdown","id":"9f770504","metadata":{"id":"9f770504"},"source":["# Introduction"]},{"cell_type":"markdown","id":"6fb2c2b3","metadata":{"id":"6fb2c2b3"},"source":["\n","This project prepares you to make multi-layered neural networks by analyzing the computational capabilities of single neurons in **feed forward networks**, by implementing basic **pattern recognition**. The project contains a primer about **simple artificial neurons**, and suggests implementations of this neuron in python.\n","\n","In the lecture you have learned that a neuron is said to have a receptive field for some **feature** if the neuron fires preferentially when that feature is presented to the organism.\n","\n","In this project, you will **manually design and tune a network** that can recognize oriented bars, thereby producing neurons that have an **orientation preference** and thus, a **tuning curve**. This should potentiate your **intuition** about how network weights **represent** preferred stimuli. You will learn how networks of neurons  can represent multiple preferred stimuli simultaneously, and thus you will learn first hand about how neurons conduct 'parallel distributed processing' (**PDP**). The insights herein are at the core of modern **deep learning** and **convolutional neural networks**.\n","\n","This project sets the stage for the introduction to backpropagation of errors in multilayered networks. You will learn about the **perceptron learning rule**, a supervised learning method that is used to optimize weights of single layer feed forward networks.\n"]},{"cell_type":"markdown","id":"48bd9315","metadata":{"id":"48bd9315"},"source":["## Key Terms\n"]},{"cell_type":"markdown","id":"a5ba730c","metadata":{"id":"a5ba730c"},"source":["- **Pattern Recognition**: The ability to ascribe a label (category) to a given pattern (input, e.g., pixel image).\n","- **Receptive Fields**: A neuron is said to have a receptive field for a stimulus, when it has a specific response to that stimulus.\n","- **Heavyside step function**: A function that returns 1 when the argument is positive and zero otherwise.\n","- **Feed Forward Neural Networks (FFNN)**: Networks that are strictly forward (no recursion).\n","- **Weight vectors**: A set of multiplicative values that apply to input.\n","- **Bias**: A value that represents baseline activity.\n","- **McCulloch-Pitts Neurons**: A neuron model that outputs 1 when a threshold is reached and zero otherwise."]},{"cell_type":"markdown","id":"643d1d59","metadata":{"id":"643d1d59"},"source":["## Learning Objectives"]},{"cell_type":"markdown","id":"03fddd7c","metadata":{"id":"03fddd7c"},"source":["- Students can code a simple \"McCulloch-Pitts\" binary neuron in python.\n","- Students understand how the operation of a simple neuron is represented by the dot product and a threshold (\"Heaviside\") function.\n","- Students are able to manually tune weights for a feed forward network to recognize oriented bars.\n","- Students develop intuition about the XOR problem and why feed forward neural networks with only one layer cannot solve it.\n","- Students learn how  to use python dictionaries to 'contain' stimuli.\n","- Students learn about a 'confusion matrix' to examine the outputs of their manually tuned classifiers.\n","---\n","\n","This sets the stage for:\n","\n","---\n","\n","- Students can provide reasons for the increase of receptive field complexity (and of receptive field size) in multilayer neural networks;\n","- Students can explain the role of convolutional layers in image recognition.\n","- Students can use an error to train a perceptron to recognize a set of patterns.\n","- Students know how to use backpropagation to train a network of classifiers.\n","- Understanding the role of multiple layers in solving the XOR problem."]},{"cell_type":"markdown","id":"6781547c","metadata":{"id":"6781547c"},"source":["## Initialization"]},{"cell_type":"code","execution_count":null,"id":"056bee59","metadata":{"id":"056bee59"},"outputs":[],"source":["# always run this cell (shift+enter) to load relevant libraries\n","\n","import matplotlib.pyplot as plt\n","from numpy import * # note that we're importing all of numpy into base namespace\n"]},{"cell_type":"markdown","id":"d15052c0","metadata":{"id":"d15052c0"},"source":["# A Receptive Field\n","\n","In biology, a neuron is said to have a **receptive field** for some **feature** if the neuron fires preferentially when that features is presented to the animal. In the case of sensory receptive fields, one can have preferences for auditory frequencies, location of stimuli in visual, location of stimuli in tactile fields, visual features, specific odors, and the list goes on.\n","\n","Neurons that selectively respond to a certain stimuli are effectively categorizing said stimuli. A neuron that responds to a certain feature of a stimulus is said *to have a receptive field* for that feature. In the study of artificial neural networks, the accepted way to think about a neuron's receptive field is as a **classifier**: it fires the most when its **preferred stimulus** is present.\n","\n","<center>\n","<img  src=https://qph.fs.quoracdn.net/main-qimg-957451c779574bcb4f9222c7801fcc11.webp align=\"middle\" width=\"350\">\n","</center>\n","\n","You probably recognize this picture above as Hubel and Wiesel's demonstration of selection prefererence in V1 cortex of the cat."]},{"cell_type":"markdown","id":"817fc5f1","metadata":{"id":"817fc5f1"},"source":["# A Simple Artificial Neuron"]},{"cell_type":"markdown","id":"c20bdffb","metadata":{"id":"c20bdffb"},"source":["\n","In the most general case an **artificial neuron** takes in some **input** (such as currents from excitatory post-synaptic currents, EPSCs) and performs some **computation** on it (e.g., a threshold).\n","\n","In the most basic scenario, the neuron **sums weighted inputs** and **applies a threshold**. The neuron is said to be active (spike) if the threshold is reached. Though this is simpler, it is the same idea underlying the 1D integrate and fire neuron (IF).\n","\n","One of the earliest models of the neuron was the **McCulloch-Pitts** unit. In that conception, many simplifications were made:\n","\n","0. A neuron either fires or does not, depending on whether the inputs cross a threshold value. The artificial Neuron produces 1/0 outputs.\n","1. The inputs to a neuron are represented by real numbers representing synaptic currents. Positive for excitatory inputs and negative for inhibitory inputs.\n","2. The inputs are weighted (multiplied by a weight). This represents a synaptic conductance. The inputs are then summed: that is, the neuron's inputs are **linearly combined**.\n","3. The threshold is a constant. It represents Sodium channels positive feedback driving a spike.\n","4. Inhibition works in the same way than excitation: equal amounts of inhibition and excitation cancel each other out.\n","5. The input to the neuron is a static pattern.\n","\n","Even though many of these simplifications are at odds with biological detail, they are a power explanatory tool for a neuron's information processing.\n"]},{"cell_type":"markdown","id":"7ee43ba3","metadata":{"id":"7ee43ba3"},"source":["---\n","**A Linear Combination**\n","\n"," The \"weighted sum of inputs\" is referred to as a **linear combination**.\n","\n","> **Example:**\n","> - if inputs are $a_1$, $a_2$, $a_3$ (or the 'vector' $\\bf{a}$)\n","> and weights are $w_1$, $w_2$, $w_3$ (or the 'vector' $\\bf{w}$)\n","> a linear combination is simply\n","> $a_1 w_1 + a_2 w_2 + a_3 w_3$\n","\n","This can be represented by [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication):\n","\n","\n","$$\\begin{bmatrix}\n","w_1 & w_2 & w_3\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","a_1 \\\\\n","a_2 \\\\\n","a_3\n","\\end{bmatrix} = \\bf{w}^T \\bf{a}$$\n","\n","\n","which can be equivalently written in [dot product](https://en.wikipedia.org/wiki/Dot_product) form:\n","\n","\n","$$\\bf{w} \\cdot \\bf{a}$$\n","\n","---"]},{"cell_type":"markdown","id":"fa33408b","metadata":{"id":"fa33408b"},"source":["For a single McCulloch-Pitts artificial neuron receiving from N sources, we can frame the inputs ($a_n$) to the neuron ($\\mathbf{a}$) as a vector with N entries. The input from each source is multiplied by its corresponding weight. The weight is a vector $\\mathbf{w}$ with N weights (also called 'entries'). Weights can be positive (excitatory) or negative (inhibitory). Inputs to the neuron are multiplied by the weights and summed together (\"linear combination\"). If the result of this operation is larger than a threshold (often 0), we say the neuron is active.\n","\n","> One can think about the inputs as frequency or current of synaptic potentials.The weights representing the amplitude of the post-synaptic current (EPSC or IPSC).\n","\n","These operations are conveniently represented via matrix multiplication (or equivalently, for 1D vectors, the dot product). It is important to note that **matrix multiplication multiplies a row vector by a column vector**. The output of matrix multiplication of a column vector by a row vector is a number (a \"scalar\").\n","\n","Thus, the activity of a neuron receiving inputs $\\bf{x}$ weighted by weights $\\bf{w}$, is a [scalar](https://en.wikipedia.org/wiki/Scalar) representing its momentary activity.\n","\n","---\n","\n","$$\n","y = \\mathbf{w} ^T \\mathbf{x}\n","$$\n","\n","---\n","\n","In here, the **T** superscript indicates **transpose**, making $\\bf{w}$  a column vector. **PAY CLOSE ATTENTION TO THE ORDER** in which $\\mathbf{w}^T$ and $\\bf{x}$ appear. Some authors use the reversed convention, in which the weight vector is a column vector following the activity vector.\n","\n","The *threshold* of the neuron neuron is the minimum value of summed inputs that results on its activation. A threshold is 'subtracted' from the summed inputs. Equivalently, this is sometimes added to the neuron representing *intrinsic activity*, or the *baseline firing rate* of the neuron, in which case it is called a *bias* (often represented as a $\\theta$, also a scalar). Note that a *Bias* has a positive sign, and *threshold* has a negative sign.\n","\n","---\n","\n","$$\n","y = \\mathbf{w} ^T \\mathbf{x} - \\theta\n","$$\n","\n","---\n"]},{"cell_type":"markdown","id":"cfa9b93c","metadata":{"id":"cfa9b93c"},"source":["### Heaviside Step Function\n"]},{"cell_type":"markdown","id":"7b1d20eb","metadata":{"id":"7b1d20eb"},"source":["McCulloch-Pitts neurons either spike (1), or they do not (0) (but note that some real neurons have graded activity). Therefore, the only way to change propagation of information is changing the frequency of spikes along the axon, or firing rate. We can say that these neurons are **binary**, or **boolean**.\n","\n","A threshold function that distinguishes positive from negative ( *output one if input is larger than threshold*) is often called the Heaviside step function. This is in honor of [Olivier Heaviside](https://en.wikipedia.org/wiki/Oliver_Heaviside), who was a self-taught electrical engineer and mathematician who worked on circuits and logic gates.\n","\n","---\n","\n","$$\n","\\begin{split}H(x) = \\begin{cases} 0, & x < 0, \\\\ 1, &x \\ge 0. \\end{cases}\\end{split}\n","$$\n","\n","And so, a McCulloch-Pitts neuron with a Heaviside function can be written as\n","\n","$$\n","\\begin{split}H(\\mathbf{w}^T \\mathbf{x}-\\theta) = \\begin{cases} 0, & \\mathbf{w}^T \\mathbf{x} < \\theta, \\\\ 1, & \\mathbf{w}^T \\mathbf{x} \\ge \\theta. \\end{cases}\\end{split}\n","$$\n","\n","where $\\theta$ is the threshold (also 'bias') of the neuron.\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","id":"62d757c3","metadata":{"id":"62d757c3"},"source":["Finally, the (McCulloch-Pitts) neuron can either fire (1) or not (0), if the sum is larger than a threshold ($\\theta$), a value representing the minimum current that the neuron requires to activate sodium channels.\n","\n","---\n","\n","$$\n","H(\\mathbf{w} ^T \\mathbf{x} - \\theta )\n","$$\n","\n","---\n","Where $H(x)$ is a the **Heaviside function**, which checks whether $x > 0$ and responds with a $1$ if yes and a $0$ if no.\n","\n","It is easy to define a function that acts like this simple neuron with *python*. If ```inputs``` is the $x$  vector, weights is the $w$ a vector, bias $\\theta$ is a scalar (single number), a neuron can be written like this:"]},{"cell_type":"markdown","id":"0c0cac60","metadata":{"id":"0c0cac60"},"source":["*If you do not know how to define a function in python, check [this link](https://pythonbasics.org/functions/).*\n","\n","```python\n","def Heaviside(x):\n","    if x >= 0:\n","      y = 1\n","    else\n","      y = 0\n","    return y\n","```"]},{"cell_type":"code","execution_count":null,"id":"a2c9fd55","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1759818237768,"user":{"displayName":"Mario Negrello","userId":"10136788594790905986"},"user_tz":-120},"id":"a2c9fd55","outputId":"b07bd9e7-3a2a-47f2-d06c-1598ea4463e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["the Heaviside of 0.3 is 1\n","the Heaviside of 0.0 is 1\n","the Heaviside of -1.4 is 0\n","the Heaviside of -inf is 0\n","the Heaviside of inf is 1\n","the Heaviside of nan is 0\n"]}],"source":["#@title ### Sample Code\n","\n","# example:\n","# we define the function above and call it with for example, x=-1 or x=1\n","\n","def Heaviside(x):\n","    if x >= 0:\n","      y = 1\n","    else:\n","      y = 0\n","    return y\n","\n","# test the function with various values of x\n","x = 0.3\n","print(\"the Heaviside of \"+str(x)+\" is \"+str(Heaviside(x)))\n","\n","x = 0.\n","print(\"the Heaviside of \"+str(x)+\" is \"+str(Heaviside(x)))\n","\n","x = -1.4\n","print(\"the Heaviside of \"+str(x)+\" is \" +str(Heaviside(x)))\n","\n","x = -inf # where inf represents infinity\n","print(\"the Heaviside of \"+str(x)+\" is \" +str(Heaviside(x)))\n","\n","x = inf\n","print(\"the Heaviside of \"+str(x)+\" is \" +str(Heaviside(x)))\n","\n","x = nan\n","print(\"the Heaviside of \"+str(x)+\" is \" +str(Heaviside(x)))"]},{"cell_type":"markdown","id":"4094a94b","metadata":{"id":"4094a94b"},"source":["### Exercise: A Single Neuron\n","Create a working function for a single ```neuron(...)``` according to the definition above. The neuron takes in a vector of inputs $\\mathbf{x}$, a weight vector $\\mathbf{w}$, and a threshold ($\\theta$) and outputs `0` or `1`."]},{"cell_type":"code","execution_count":null,"id":"31d45806","metadata":{"id":"31d45806","executionInfo":{"status":"error","timestamp":1696324212593,"user_tz":-120,"elapsed":17,"user":{"displayName":"Mario Negrello","userId":"10136788594790905986"}},"outputId":"dd855738-39a3-49d9-bdd9-7c7c9d58dab7","colab":{"base_uri":"https://localhost:8080/","height":159}},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-72b7bcbfbaf8>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    w =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["#@title ### Your Solution\n","\n","# 1. Define an input vector x, weigth vector w, and a threshold theta\n","\n","w =\n","x =\n","theta =\n","\n","# 2. Define a python function called neuron that takes in weights, inputs and a threshold\n","# and computes the Heaviside output of the weighted sum of inputs.\n","# 2.1 use linear algebra to multiply vector and matrix to avoid a for loop\n","\n","# use the template below to define your neuron\n","# def neuron(x,w,theta)\n","# ...\n","#   return y\n","\n","def neuron(x,w,theta):\n","  y=\n","\n","  return 1. * (y >= 0)\n","\n","# 3. test it on your stimuli\n","output = neuron(x,w,theta)\n","print(output)"]},{"cell_type":"code","execution_count":null,"id":"14b78251","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1696324219093,"user":{"displayName":"Mario Negrello","userId":"10136788594790905986"},"user_tz":-120},"id":"14b78251","outputId":"2bcbcb97-3ca5-49cd-b3b3-127b41b7d3b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["the threshold value theta is 0.5\n","the weight vector w is [[0.36792241]\n"," [0.20647144]\n"," [0.72866881]]\n","the input vector x is [[-0.02816827]\n"," [-1.8283842 ]\n"," [-0.14292003]]\n","the output of your neuron is [[0.]]\n"]}],"source":["#@title ### Sample Code\n","\n","# create some inputs\n","x = random.uniform(-2,2,size=(3,1))\n","w = random.uniform(0,1,size=(3,1))\n","theta = 0.5\n","\n","\n","def neuron(x,w,theta):\n","    # linear combination using @ for the dot product\n","    cell_body_sum = w.T@x - theta\n","    # and fire (output)\n","\n","    # multiply by 1. to 'cast' result as float (type)\n","    return 1. * (cell_body_sum >= 0)\n","\n","output = neuron(x,w,theta)\n","print(\"the threshold value theta is \" +str(theta))\n","print(\"the weight vector w is \"+str(w))\n","print(\"the input vector x is \" +str(x))\n","print(\"the output of your neuron is \"+str(output))"]},{"cell_type":"markdown","id":"9525fa45","metadata":{"id":"9525fa45"},"source":["*If you try to run our sample code multiple times does the output of the neuron change?* yes, as the input is generated as vectors with random uniform distributed values"]},{"cell_type":"markdown","id":"686914df","metadata":{"id":"686914df"},"source":["Evidently, the simplifications for this neuron are on the *heavy side* (poor pun). Biological neurons are in dynamical systems. For exmple, neurons often are sensitive for timing between the synaptic potentials, and dendrites perform non-linear operations such as amplifying input signals. Nevertheless, under many situations, even the most complex neurons can be thought of as performing nature's version of this yes/no operation. Case in point, receptive fields of neurons are compellingly represented by the basic operations of an articicial neuron as defined above.\n"]},{"cell_type":"markdown","id":"69a74543","metadata":{"id":"69a74543"},"source":["\n","## The Binary Neuron as a Linear Classifier  \n"]},{"cell_type":"markdown","id":"becc940a","metadata":{"id":"becc940a"},"source":["\n","The simple artificial neuron described above performs the function of a **classifier**. It classifies its inputs into those that activate  it, and those that do not.\n","\n","For example, a classifier could tell you whether inputs belong to the class of penguin. Or it could classify things that happen together: this **AND** that have to be the case. This is the *boolean* function **AND**. A neuron with two inputs that computes an **AND** function returns `1` (`True`) if both input one **AND** input two are `1`. It should says `0` (`False`) if any of the inputs are absent.\n","\n","With properly tuned weights and bias, a linear classifier can compute most [logical gates](https://en.wikipedia.org/wiki/Logic_gate), such as (AND, OR, NOT, NAND)."]},{"cell_type":"markdown","id":"ac1f5fc6","metadata":{"id":"ac1f5fc6"},"source":["### An **AND** gate"]},{"cell_type":"code","execution_count":null,"id":"9b8dc857","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":447,"status":"ok","timestamp":1696324229304,"user":{"displayName":"Mario Negrello","userId":"10136788594790905986"},"user_tz":-120},"id":"9b8dc857","outputId":"bd6ba1d2-da6e-4084-a50a-4fea7643f439"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":5}],"source":["def AND(a,b):\n","    if a == 1 and b == 1:\n","        return True\n","    else:\n","        return False\n","\n","print(AND(0,1))\n","\n","AND(1,1)\n","\n"]},{"cell_type":"markdown","id":"a13df669","metadata":{"id":"a13df669"},"source":["Ask yourself: *can you find weights and bias to calculate an **AND** gate with the artificial neuron above?*"]},{"cell_type":"markdown","id":"74443c15","metadata":{"id":"74443c15"},"source":["### The **XOR** gate"]},{"cell_type":"code","execution_count":null,"id":"768373f0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1696324231632,"user":{"displayName":"Mario Negrello","userId":"10136788594790905986"},"user_tz":-120},"id":"768373f0","outputId":"84a9676d-372c-4cba-d38e-ae402caf62b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":6}],"source":["def XOR(a,b):\n","    if a != b:\n","        return 1\n","    else:\n","        return 0\n","XOR(1,0)"]},{"cell_type":"markdown","id":"7b46d9fa","metadata":{"id":"7b46d9fa"},"source":["----\n","\n","#### Question\n","\n","- One of the two logical functions above cannot be performed by the artificial neuron defined earlier. Can you guess which one? Explain your answer.\n","\n","---"]},{"cell_type":"markdown","id":"0c4ccf48","metadata":{"id":"0c4ccf48"},"source":["#### Answer\n","\n","*Double click this cell and type your answer here*\n"]},{"cell_type":"markdown","id":"891ac3f3","metadata":{"id":"891ac3f3"},"source":["# FFNNs: Encoding receptive fields\n"]},{"cell_type":"markdown","id":"a8d336b8","metadata":{"id":"a8d336b8"},"source":["\n","## Exercise: Manually Tune The Weights of a Neuron\n"]},{"cell_type":"markdown","id":"d47358d7","metadata":{"id":"d47358d7"},"source":["\n","In this exercise, you are expected to **manually tune a neuron by selecting weights vector** (and maybe a bias vector), so that the neuron is selective for a feature, that is, it only responds to a specific stimulus. The purpose of this exercise is to build intuition about what do the weights have to look like for a neuron to respond selectively to certain patterns.\n","\n","> **Roadmap**:\n","- create vectors representing stimulus (using dictionaries)\n","- choose weights for a single output neuron such that the neuron says ```1``` to the preferred stimulus and ```0``` to other stimuli.\n","- choose weights for more output neurons, one for each stimulus orientation.\n","- test the classification\n"]},{"cell_type":"markdown","id":"e0e0f1cd","metadata":{"id":"e0e0f1cd"},"source":["\n","## Creating Stimuli"]},{"cell_type":"markdown","id":"3a5a7948","metadata":{"id":"3a5a7948"},"source":["\n","Before we start selecting weights, we must create inputs to our network. For this exercise we will be creating stimuli that are **oriented bars** representing how light activates a tiny patch of the retina, of say, 3 x 3 rods (photoreceptors that are responsive to luminance).The cells involved with processing visual data can be very specific with their responses. As Hubel and Wiesel discovered, in the visual cortex (V1)  some neurons only fire (action potentials) for bars at certain angles.\n","\n","As you remember, the retina has a dense array of rods, but here we are considering only receptors that are really close to each other, a very small \"visual angle\". We will represent active photoreceptors with `1` and silent photoreceptors with `0`.\n","\n","\n","Before we start the process of selecting weights, it will be useful to create test inputs to our network. In the sample code below we will be creating oriented bars that represent how light activates a small patch of the retina, of say, 3 x 3 pixels. Thus, the initial stimulus is a 3x3 bitmap. We will be creating these stimuli as numpy arrays introduced into python's data type 'dictionary'.\n"]},{"cell_type":"markdown","id":"e2ccc0fe","metadata":{"id":"e2ccc0fe"},"source":["### Example: Creating Stimuli"]},{"cell_type":"markdown","id":"18bcb509","metadata":{"id":"18bcb509"},"source":["Using numpy's `array()` we create stimuli with zeros for dark pixels and ones for bright pixels. Create 4 stimuli for a vertical bar, a horizontal bar, and two diagonal bars (left and right slant, i.e., / and \\), all of them using three 'on' pixels centered in the patch. We are going to present these stimuli to the network, to test whether our choice of weights works."]},{"cell_type":"code","execution_count":null,"id":"a79fd30d","metadata":{"id":"a79fd30d"},"outputs":[],"source":["#@title ### Sample Code\n","\n","# want to represent how light activates a part of the retina,\n","# so the '1' is bright (on) and '0' is dark.\n","#\n","# create 3x3 matrices representing the way light activates\n","# pixels in a small 2D patch\n","S1 = array([[0,1,0], [0,1,0], [0,1,0]])\n","S2 = array([[0,0,0], [1,1,1], [0,0,0]])\n","S3 = array([[1,0,0], [0,1,0], [0,0,1]])\n","S4 = array([[0,0,1], [0,1,0], [1,0,0]])\n","\n","\n","plt.figure\n","plt.subplot(1,4,1), plt.imshow(S1,cmap='gist_gray');\n","plt.subplot(1,4,2), plt.imshow(S2,cmap='gist_gray');\n","plt.subplot(1,4,3), plt.imshow(S3,cmap='gist_gray');\n","plt.subplot(1,4,4), plt.imshow(S4,cmap='gist_gray');"]},{"cell_type":"markdown","id":"e37eea7a","metadata":{"id":"e37eea7a"},"source":["### Transforming Stimuli Matrices Into Input Vectors"]},{"cell_type":"markdown","id":"38d02960","metadata":{"id":"38d02960"},"source":["Above we have created some matrices representing the 2D patch of retinal. But the inputs to a neuron do not necessarily maintain the spatial orientation of the incoming synapses (that is, it is not necessarily the case that the input synapse arriving at the top of the neuron should come from the retinal pixel at the top -- in other words, *topography* is not kept).\n","\n","To pass on the stimlus to our networks we have to transform them in a list (a vector, to perform linear combination). So we need to rearrange the elements of the stimulus matrices as **input vectors**.\n","\n","We will want to take each of the entries of our stimuli dictionary and reshape the matrices as 1D vectors. (i.e., **We need to vectorize the matrices**)."]},{"cell_type":"code","execution_count":null,"id":"822018f8","metadata":{"id":"822018f8"},"outputs":[],"source":["#@title ### Sample Code\n","\n","# one way to reshape all matrices into vectors is to use flatten\n","v_v = S1.flatten()\n","v_h = S2.flatten()\n","v_dl = S3.flatten()\n","v_dr = S4.flatten()\n","\n","print(v_v)\n","print(v_h)\n","print(v_dl)\n","print(v_dr)"]},{"cell_type":"markdown","id":"afcd67ed","metadata":{"id":"afcd67ed"},"source":["------------\n","> #### Interlude: Dictionaries\n",">\n","> Python works with several different datatypes; lists, dictionaries, tuples, arrays, to name a few. A **dictionary** is datatype that collects and can be indexed via 'keys' (a string for example). Dictionaries are written with curly brackets. While other compound data types have only value as an element, a dictionary has a ```\"key\": value ``` pair. Simply put, you can use a dictionary to retrieve 'values' that matches a given `key`.\n",">\n","> Each key is separated from its value by a colon `(:)`, the items are separated by commas, and the whole thing is enclosed in curly braces. An empty dictionary without any items is written with just two curly braces, like this: `{}`.\n",">\n","> Keys are unique within a dictionary while values may not be. The values of a dictionary can be of any type, but the keys must be of an immutable data type such as strings, numbers, or tuples.\n",">\n","> ```python\n","numbers = {'one':1, 'two':2, 'three':3}\n","```\n","> and so calling our  example as such\n","> ```python\n","numbers['one']\n","```\n","> returns a 1.\n","------------\n"]},{"cell_type":"markdown","id":"183b15ea","metadata":{"id":"183b15ea"},"source":["#### Sample Code: Stimulus Dictionary"]},{"cell_type":"code","execution_count":null,"id":"df21b9d1","metadata":{"id":"df21b9d1"},"outputs":[],"source":["# Here we create a dictionary with the stimuli (so that it becomes easy to manage them)\n","stimuli = {'horizontal':v_h, 'vertical':v_v,'diagonal_left':v_dl, 'diagonal_right':v_dr}\n","\n","# iterate through values in the dictionary\n","# more ways to do it here: https://realpython.com/iterate-through-dictionary-python/#iterating-through-keys-directly\n","for key in stimuli:\n","    print(key, '->', stimuli[key])"]},{"cell_type":"markdown","id":"f2941fe6","metadata":{"id":"f2941fe6"},"source":["## Designing Weights for a Classifier Neuron\n","\n","Here we will like to design weights and choose a threshold such that the output neuron has a preference for  **vertical oriented bar**, smack in the middle of the receptive field of the neuron (see the third neuron in our code above). The neuron SHOULD NOT FIRE to any of the other stimuli."]},{"cell_type":"markdown","id":"daa3ee6f","metadata":{"id":"daa3ee6f"},"source":["### Select Weigths for a Vertically Tuned Neuron"]},{"cell_type":"markdown","id":"89ca1384","metadata":{"id":"89ca1384"},"source":["Select a weight vector and theta such that your neuron is active when a vertical line is presented to the neuron's receptive field.\n","\n","Fill in the weights and theta in the code cell below.\n","\n","Test the output of your neuron with all the stimuli in the stimulus dictionary (created above)."]},{"cell_type":"code","execution_count":null,"id":"99345006","metadata":{"id":"99345006","cellView":"form"},"outputs":[],"source":["#@title ### Exercise\n","\n","# DEFINE your weight vector\n","w_vertical_neuron =\n","\n","# CHOOSE a threshold\n","theta =\n","\n","# SELECT the vertical stimulus from dictionary\n","vertical_stimulus = stimuli['vertical']\n","\n","# USE the neuron function you defined priorly (linear combination + heaviside function)\n","\n","# CALL the function with your stimulus and chosen weights\n","test1 = neuron(vertical_stimulus,w_vertical_neuron,theta)\n","print(f'test1 = {bool(test1)}')\n","\n","# TEST the function for every orientation and check that it responds adequatly\n","test_1 = neuron(stimuli['horizontal'],w_vertical_neuron,theta)\n","test_2 = neuron(stimuli['diagonal_right'],w_vertical_neuron,theta)\n","test_3 = neuron(stimuli['diagonal_left'],w_vertical_neuron,theta)\n","\n","print(f'test_1 = {bool(test_1)}')\n","print(f'test_2 = {bool(test_2)}')\n","print(f'test_3 = {bool(test_3)}')"]},{"cell_type":"markdown","id":"22408412","metadata":{"id":"22408412"},"source":["*if test1 is True and all others are false, congratulations! Else go back to the weight design phase.*"]},{"cell_type":"markdown","id":"777880e5","metadata":{"id":"777880e5"},"source":["---\n","#### Question\n","- What would happen if the contrast between active and inactive pixels would be lower? For example, what if our `on` pixels had a value of `0.6` and the `off` pixels a valu of `0.7`? Would your weights still work? Can you think of a strategy to make your neuron more general?\n","---"]},{"cell_type":"markdown","source":["#### Answer\n","\n","*Double click this cell and type your answer here*"],"metadata":{"id":"77TLPlEWoMDN"},"id":"77TLPlEWoMDN"},{"cell_type":"markdown","id":"b87aa4c9","metadata":{"id":"b87aa4c9"},"source":["## Feed Forward Network of Classifiers"]},{"cell_type":"markdown","id":"df3d07b6","metadata":{"id":"df3d07b6"},"source":["\n","Our stimuli are coming from 9 pixels and mapping on a single ouptut neuron that recognizes a vertical bar.\n","\n","What if we want four output neurons, each with a different prefered orientation?\n","\n","Essentially, the operation for each neuron is the same linear combination of inputs.\n","\n","$$ \\bf{W} \\bf{x}$$\n","\n","But now $\\bf{W}$ is a **weight matrix** that contains one (row) weight vector for each of the output neurons. To produce the matrix we simply stack four weigth vectors, each with weights standing for different preferences."]},{"cell_type":"markdown","id":"a2c72a58","metadata":{"id":"a2c72a58","lines_to_next_cell":0},"source":["### Extending the Network\n","\n","1. Extend your `neuron()` function to compute the output of a one layer feed forward neural network.\n","2. Define weight vectors, each of which with a particular orientaion preference.\n","3. Concatenate these into a weight matrix.\n","4. Test the output of your network for all of the previously prepared stimuli."]},{"cell_type":"markdown","id":"b8a1cab0","metadata":{"id":"b8a1cab0"},"source":["\n"]},{"cell_type":"code","execution_count":null,"id":"5d4fd7da","metadata":{"id":"5d4fd7da"},"outputs":[],"source":["#@title ### Sample Code\n","\n","# DEFINE the weight vectors (as np.arrays) for different output neurons\n","w_vertical = array([-1,2,-1,-1,3,-1,-1,2,-1])\n","w_horizontal = array([-1,-1,-1,2,3,2,-1,-1,-1])\n","w_diagonal_L = array([2,-1,-1,-1,3,-1,-1,-1,2])\n","w_diagonal_R = array([-1,-1,2,-1,3,-1,2,-1,-1])\n","\n","# DEFINE a threshold\n","theta = 6\n","\n","# CONCATENATE the weight vectors into a weight matrix\n","## https://stackoverflow.com/questions/20978757/how-to-append-a-vector-to-a-matrix-in-python\n","# weight_matrix = np.c_[w_v , w_h , w_dr , w_dl]\n","\n","weight_matrix = c_[w_vertical , w_horizontal , w_diagonal_R , w_diagonal_L]\n","\n","\n","# CHECK that your matrix has the right shape:\n","# print(weight_matrix.shape)\n","\n","# EXTEND your single neuron function to compute the output of a feed forward neural network\n","\n","def FFNN(W, I, theta):\n","  # If W is the weigth matrix and\n","  # I is the input vector\n","  # theta is the threshold\n","\n","  # to compute the output\n","  # use matrix multiplication (\"@\" in numpy) and apply a threshold (heaviside)\n","  neuronlist=[]\n","\n","  for i in range(W.shape[0]):\n","    weights=W[i,:]\n","    y=weights.T@I-theta\n","    neuronlist.append(1* (y >= 0))\n","  return neuronlist\n","\n","# TEST your network on all stimuli\n","\n","# you can use our function to do so:\n","# compute outputs for all stimuli and all neurons\n","print('The order of neuron preferences are: Vertical, Horizontal, Diagonal_right and Diagonal_left\\n')\n","for key, value in stimuli.items():\n","  print(f'For {key} stimulus')\n","  print(f'{FFNN(weight_matrix.T, stimuli[key], theta)} \\n')\n","\n"]},{"cell_type":"markdown","id":"981a7875","metadata":{"id":"981a7875"},"source":["\n","### Exercise / Challenge: A Cross\n","\n","Design an weigth vector for an output neuron that:\n","- returns `1` for a vertical_bar stimulus\n","- returns `1` for a horizontal_bar stimulus\n","- returns `0` for a cross stimulus\n","\n","We defined the cross for your convenience\n","```python:\n","cross = array([0,1,0],[1,1,1],[0,1,0])\n","```\n","\n","For this exercise add this cross stimulus to your stimulus dictionary and add another weight vector that performs as above.\n"]},{"cell_type":"markdown","id":"0b5ffcc7","metadata":{"id":"0b5ffcc7"},"source":["### Your Code"]},{"cell_type":"code","execution_count":null,"id":"83f1b5c8","metadata":{"id":"83f1b5c8"},"outputs":[],"source":["# Your Code\n","cross = array([[0,1,0],[1,1,1],[0,1,0]])\n","stimuli['cross']=cross.flatten()\n","plt.imshow(cross,cmap='gist_gray')\n","\n","crossweights=array([-1,-3,-1,-3,3,2,-1,2,-1])\n","theta=2\n","weight_matrix_cross=c_[weight_matrix, crossweights]\n","print(\"The order of neuron preferences are: Vertical, Horizontal, Diagonal_right and Diagonal_left, cross\\n\")\n","for key, value in stimuli.items():\n","  print(f'For {key} stimulus')\n","  print(f'{FFNN(weight_matrix_cross.T, stimuli[key], theta)} \\n')\n"]},{"cell_type":"markdown","id":"84ac25a1","metadata":{"id":"84ac25a1"},"source":["---\n","#### Question\n","- were you able to design a weight vector solving the challenge above? Explain your answer.\n","---"]},{"cell_type":"markdown","source":["#### Answer\n","\n","*Double click this cell and type your answer here*"],"metadata":{"id":"YNj2HSf-oOtP"},"id":"YNj2HSf-oOtP"},{"cell_type":"markdown","id":"2b690026","metadata":{"id":"2b690026"},"source":["## Testing Classification Accuracy"]},{"cell_type":"markdown","id":"eae88db0","metadata":{"id":"eae88db0"},"source":["A neuron with a receptive field for a horizontal bar, may also fire when the bar is not perfectly horizontal.\n","\n","We introduce a **confusion matrix**, which is a way to see how well a network is able to classify stimuli. Confusion matrices are used to test the performance of a network that classifies stimulus (a 'model'). Confusion matrices count the number of times that a certain stimulus was assigned to a given category. That is, they display the label given by the network as a function of the true label. Note: the confusion matrix can only be used if one has 'true labels' for every single stimulus being presented, that is, if the dataset is **supervised**.\n","\n","Below is an example of a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for image classification, that tries to distinguish Dogs from Rabbits and Cats. In the case of this network, we have 'trained it' to try and see if a picture is of a cat or a dog.\n","\n","\n","![](https://i.stack.imgur.com/Rz5ol.jpg)\n","\n","<!-- another, for  standard one\n","\n","![](https://datatofish.com/wp-content/uploads/2018/12/003_cm.png)\n"," -->\n"]},{"cell_type":"markdown","id":"1573c033","metadata":{"id":"1573c033"},"source":["---\n","#### Questions\n","- How many rows and columns will be in a confusion matrix that attempts to classifies 10 stimuli in 4 categories?\n","- Can a confusion matrix have more rows than categories? Explain your answer.\n","- According to the confusion matrix above, what is the most common mistake of the classifier?\n","---"]},{"cell_type":"markdown","source":["#### Answer\n","\n","*Double click this cell and type your answer here*"],"metadata":{"id":"NAcHvZdUoRhB"},"id":"NAcHvZdUoRhB"},{"cell_type":"markdown","id":"e965a0bf","metadata":{"id":"e965a0bf"},"source":["## Multiple Layers: Convolutions\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"31b3301c","metadata":{"id":"31b3301c"},"source":["The term receptive field is also used in the context of neural networks, where it refers to the region in the **input space** that affects a particular neuron in the network.\n","\n","\n","\n"," In deep learning newtorks (receptive fields are consequences of **convolutional filters** are used to preprocess a large image by decomposing that image in multiple features. This type of model architecture, which attempts to mimick the hierarchical way visual processing (and pre-processing in other senses), is called **convolutional neural network**.\n","\n","A clever trick is introduced; instead of connecting each neuron in one layer to each other neuron of the next, the neurons are organised as such accoring to their spatial relationships in the input data. That is, each neuron in the convolutional layer receives data from only a *local patch* of inputs. Neurons in one convolutional layer will perform the same kind of operation, that is, they will extract the same kind of feature.\n","\n","This patch, determined by which neurons the convolutional layer receives from, is called the (local) receptive field.\n","\n","The feed forward neural networks we just tuned can in fact be used as convolutional filters. All which is required, is that we have a whole layer of neurons searching for a feature in all the patches of the whole picture.\n","\n","<a title=\"Vincent Dumoulin, Francesco Visin, MIT &lt;http://opensource.org/licenses/mit-license.php&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Convolution_arithmetic_-_Full_padding_no_strides_transposed.gif\"><img width=\"128\" alt=\"Convolution arithmetic - Full padding no strides transposed\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/85/Convolution_arithmetic_-_Full_padding_no_strides_transposed.gif\"></a>\n","\n","This is a set of neurons with receptive fields of 3x3 pixels looking into a larger input picture (blue). The result of this 'convolution' layer (green), could be fed into another layer of neurons, making the network 'deeper'. Compositions of convolutional layers are thought to underly processing in the visual stream, as exemplified in the figure below.\n","\n","The 'convolutional filters', essentially the 'neuronal preferences' are indicated by small 3x3 filters, like the one you designed. The outputs of this pre-processing are indicated in the light blue rectangle.\n"]},{"cell_type":"markdown","id":"017a4087","metadata":{"id":"017a4087"},"source":["This excerpt from the \"Principles of Neuroscience (5th Ed.)\" demonstrates the notion of composition of receptive fields, deriving from Hubel and Wiesel's model of simple and complex cells of the visual cortex.\n","\n","[![Hubel and Wiesel Model](https://i.postimg.cc/ncgy9Vcj/image.png)](https://postimg.cc/QHg4R37h)\n","(from \"Principles of Neuroscience, 6th Ed. Appendix \"Neural Networks\")"]},{"cell_type":"markdown","id":"3d7c0c5b","metadata":{"id":"3d7c0c5b"},"source":["---\n","# Final Questions\n","The following questions may require some research or some out of the box thinking. You are welcome to check the available resources, or ask an AI!\n","\n","- Why is the perceptron a 'linear' classifier'?\n","- When can we say that a neuron has a receptive field?\n","- Why must the weights of a classifier 'resemble' their preferred stimulus?\n","\n","---"]},{"cell_type":"markdown","source":["#### Answers\n","\n","*Double click this cell and type your answer here*"],"metadata":{"id":"RRcXpn-_oZPf"},"id":"RRcXpn-_oZPf"},{"cell_type":"markdown","id":"649d2702","metadata":{"id":"649d2702"},"source":["# Resources\n","- [3Blue1Brown on Neural Networks](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiG_M-fm4nsAhVMCewKHQ4iD00QyCkwAHoECAQQAw&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DaircAruvnKk&usg=AOvVaw3c_pmQ67XtWSaAXtAgCxkl)\n","- [A medium post on representing networks via matrices](https://medium.com/coinmonks/representing-neural-network-with-vectors-and-matrices-c6b0e64db9fb)\n","- Convolutions, aka, [Morphological operations, in python](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html)\n","- distill.pub [Interpretability](https://distill.pub/2018/building-blocks/)\n","- http://neuralnetworksanddeeplearning.com/chap6.html"]},{"cell_type":"markdown","id":"6bf15209","metadata":{"id":"6bf15209"},"source":["#License\n","\n","<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.\n","\n","Mario Negrello, Daphne Cornelise, Elias Santoro. Reviewing and testing by many students."]},{"cell_type":"code","execution_count":null,"id":"0aab4e95","metadata":{"id":"0aab4e95"},"outputs":[],"source":[]}],"metadata":{"jupytext":{"main_language":"python"},"kernelspec":{"display_name":"Python 3","name":"python3"},"colab":{"provenance":[],"collapsed_sections":["183b15ea"],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}